{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465c003a-29cf-4cfe-a0c6-d36c04ae2b37",
   "metadata": {},
   "source": [
    "# Splitters\n",
    "* Divide a large data asset in small parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46161e-45e9-46d7-8214-bcbea10aff2e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e0018-cba4-4959-881a-0a65093d202d",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065e336-d054-412c-8a3f-1fbec63e1bcd",
   "metadata": {},
   "source": [
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dda8d4-80cf-4b8f-9981-94edda5e9911",
   "metadata": {},
   "source": [
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 002-splitters.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af743328-1bc8-4b01-85fb-fcb21c6499c2",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863dd299-0780-49ad-a1b7-b76e249350da",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **002-splitters**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e766aa-f3e2-491f-be99-d0c6b700d47a",
   "metadata": {},
   "source": [
    "## Track operations\n",
    "From now on, we can track the operations **and the cost** of this project from LangSmith:\n",
    "* [smith.langchain.com](https://smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f99504a-1b8f-4360-b342-0b81ffa06aff",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e5789-5bde-42e1-88dd-92dc8e363c24",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5514113-ddca-4ae9-9de6-0b9225b18f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef1e5c-b7e2-4a04-96c5-8f64377b8eba",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d23f4-61f5-4227-8a75-7eefde6680ee",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1998155-91de-4cbc-bc88-8d77beefb51b",
   "metadata": {},
   "source": [
    "* NOTE: Since right now is the best LLM in the market, we will use OpenAI by default. You will see how to connect with other Open Source LLMs like Llama3 or Mistral in a next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c811b84c-7054-49dd-b0ac-30f327f648a0",
   "metadata": {},
   "source": [
    "## Reminder: steps of the RAG process.\n",
    "* When you load a document, you end up with strings. Sometimes the strings will be too large to fit into the context window. In those occassions we will use the RAG technique:\n",
    "    * **Split document in small chunks**.\n",
    "    * Transform text chunks in numeric chunks (embeddings).\n",
    "    * Load embeddings to a vector database (aka vector store).\n",
    "    * Load question and retrieve the most relevant embeddings to respond it.\n",
    "    * Sent the embeddings to the LLM to format the response properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f6786d-16d2-4894-b8ef-95a7ebd19a94",
   "metadata": {},
   "source": [
    "## Splitters: divide the loaded document in small chunks of text\n",
    "* Also called \"Document Transformers\".\n",
    "* See the documentation page [here](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/).\n",
    "* See the list of built-in splitters [here](https://python.langchain.com/v0.1/docs/integrations/document_transformers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8de5c1e-1426-4cc8-8588-4ab7f1ffbcd0",
   "metadata": {},
   "source": [
    "#### Simple splitting by character: Character Splitter\n",
    "* This splits based on characters (by default \"\\n\\n\") and measure chunk length by number of characters.\n",
    "* The \"Character Splitter\" in the context of RAG (Retrieval-Augmented Generation) applications, specifically using LangChain's tools, is a method that divides text into smaller parts based on specific characters.\n",
    "* By default, it uses double newline characters (\"\\n\\n\") to identify where one chunk of text ends and another begins.\n",
    "* Each chunk is measured by its number of characters.\n",
    "* This simple splitting method is useful in RAG applications to help manage and process large blocks of text by breaking them down into manageable, smaller pieces. This can enhance the efficiency and effectiveness of the text retrieval process, which is crucial in generating accurate and contextually relevant responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db520c85-d916-4465-9a53-ee6a7c0b722d",
   "metadata": {},
   "source": [
    "Here's a simple example to illustrate how the \"Character Splitter\" works in the context of RAG applications using the default delimiter (\"\\n\\n\").\n",
    "\n",
    "#### Original Text:\n",
    "```\n",
    "Hello, welcome to our store!\n",
    "\n",
    "\\n\\nWe offer a variety of products ranging from electronics to clothing.\n",
    "\n",
    "\\n\\nOur store hours are 9 AM to 9 PM every day.\n",
    "\n",
    "\\n\\nFeel free to ask for assistance if you need help finding anything.\n",
    "```\n",
    "\n",
    "#### After Applying Character Splitter:\n",
    "1. **Chunk 1:**\n",
    "   ```\n",
    "   Hello, welcome to our store!\n",
    "   ```\n",
    "\n",
    "2. **Chunk 2:**\n",
    "   ```\n",
    "   We offer a variety of products ranging from electronics to clothing.\n",
    "   ```\n",
    "\n",
    "3. **Chunk 3:**\n",
    "   ```\n",
    "   Our store hours are 9 AM to 9 PM every day.\n",
    "   ```\n",
    "\n",
    "4. **Chunk 4:**\n",
    "   ```\n",
    "   Feel free to ask for assistance if you need help finding anything.\n",
    "   ```\n",
    "\n",
    "In this example, the text is split into four chunks based on the presence of \"\\n\\n\" between sections of text. Each chunk is a manageable size and clearly separated from the others, making it easier for a RAG system to handle and retrieve information from specific parts of the text as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aba8ec6a-9988-493f-9bc2-a1d72eb095b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./data/be-good.txt\")\n",
    "\n",
    "loaded_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9080df7c-9e81-4d2d-8713-9345e2ac8c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ecb63be-4c91-4bec-ba71-5d05dc962722",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "063a35f4-da0c-4915-978d-be3d9f70befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a1f58df-c7b1-491c-93a1-c065591b28e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.create_documents([loaded_data[0].page_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c2bf32e-3cc8-4b59-bc8d-5617b37a62fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97c9e310-597b-46aa-accf-69eeda77f7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Be good')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "297dd163-7d2c-476b-b332-c2bf232461d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9b427-4002-4995-9db4-63d5afa9403c",
   "metadata": {},
   "source": [
    "#### Splitting with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "becc89c4-ec8c-468a-af28-711184d9c75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadatas = [{\"chunk\": 0}, {\"chunk\": 1}]\n",
    "\n",
    "documents = text_splitter.create_documents(\n",
    "    [loaded_data[0].page_content, loaded_data[0].page_content], \n",
    "    metadatas=metadatas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a082af2-0913-4b8a-8e98-073e0bae2fbe",
   "metadata": {},
   "source": [
    "The `CharacterTextSplitter` from LangChain's text splitting utilities helps break down text into manageable chunks along with the option to attach metadata to each chunk. Here’s how it works in simple terms:\n",
    "\n",
    "1. **Initialization of the Splitter**: The splitter is configured with several parameters:\n",
    "   - `separator`: This is the character or string at which the text will be split. In this example, it’s set to `\"\\n\\n\"`, which means the text will be divided at every occurrence of double newlines.\n",
    "   - `chunk_size`: This determines the maximum size of each chunk in characters. Here, each chunk can be up to 1000 characters long.\n",
    "   - `chunk_overlap`: This allows chunks to overlap by a specified number of characters, set here to 200. Overlapping helps ensure that no crucial information is cut off awkwardly at the end of a chunk.\n",
    "   - `length_function`: This is typically a function to measure the length of the text; `len` simply counts the number of characters.\n",
    "   - `is_separator_regex`: This indicates whether the separator is a regular expression or not. Here, it’s `False`, meaning the separator is treated as a literal string.\n",
    "\n",
    "2. **Creating Chunks**: The `create_documents` method is used to split the text. It takes in the text (or texts) and optional metadata, and splits the text based on the defined parameters.\n",
    "\n",
    "3. **Attaching Metadata**: Optionally, metadata can be attached to each chunk to provide additional context or identifiers. For example, metadata might tag each chunk with its sequence number or categorize it based on content. In the example, metadata like `{\"chunk\": 0}` could signify that it’s the first chunk.\n",
    "\n",
    "#### Example Code Explanation:\n",
    "- **First Splitter Use**: Initially, the splitter is used to split the content of `loaded_data[0].page_content` without metadata. This will just divide the text based on the provided settings.\n",
    "- **Second Splitter Use**: The same text is then split again, but this time with metadata provided (`metadatas=[{\"chunk\": 0}, {\"chunk\": 1}]`). This second run distinguishes each chunk with additional information.\n",
    "\n",
    "#### Example Outputs:\n",
    "- **Number of Chunks**: This would typically be printed using `len(texts)`, showing how many chunks the text was split into.\n",
    "- **First Chunk**: By printing `texts[0]`, you would see the content of the first chunk.\n",
    "- **First Chunk with Metadata**: Printing `documents[0]` after the second splitting operation would show the first chunk of text along with its corresponding metadata, illustrating how metadata is associated with text chunks.\n",
    "\n",
    "This method is particularly useful in applications where chunks of text need to be processed independently, but still require some form of contextual or sequential linking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f32ee1a-dcbc-4906-9e26-ff1c00f43e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'chunk': 0}, page_content='Be good')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a892bfb2-d1ab-4e9d-a4b2-25a4a6319577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Be good' metadata={'chunk': 0}\n"
     ]
    }
   ],
   "source": [
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea671270",
   "metadata": {},
   "source": [
    "#### Recursive Character Splitter\n",
    "* This text splitter is the recommended one for generic text. \n",
    "* It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. \n",
    "* This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c24c6b-f163-47da-9282-02d3f14b947f",
   "metadata": {},
   "source": [
    "#### Simple Explanation:\n",
    "\n",
    "* The \"Recursive Character Splitter\" is a method used to divide text into smaller, more manageable chunks, designed specifically to maintain the semantic integrity of the text.\n",
    "* It operates by attempting to split the text using a list of characters in a specified order—beginning with the largest units like paragraphs, then moving to sentences, and finally to individual words if needed.\n",
    "* The default sequence for splitting is [\"\\n\\n\", \"\\n\", \" \", \"\"], which means it first tries to split the text at double newline characters to separate paragraphs, then at single newlines for any remaining large blocks, followed by spaces to isolate sentences or phrases, and finally using an empty string if finer splitting is necessary.\n",
    "* This method is particularly effective because it tries to keep text chunks as meaningful and complete as possible, ensuring that each chunk has a coherent piece of information.\n",
    "\n",
    "#### Example of Use:\n",
    "\n",
    "#### Original Text:\n",
    "```\n",
    "Hello, welcome to our store!\n",
    "\n",
    "\\n\\nWe offer a variety of products. Our range includes electronics, clothing, and home appliances.\n",
    "\\nOur staff is available to help you during store hours: 9 AM to 9 PM every day.\n",
    "```\n",
    "\n",
    "#### Applying Recursive Character Splitter:\n",
    "- First attempt with `\"\\n\\n\"`:\n",
    "  1. **Chunk 1:** `Hello, welcome to our store!`\n",
    "  2. **Chunk 2:** `We offer a variety of products. Our range includes electronics, clothing, and home appliances.\\nOur staff is available to help you during store hours: 9 AM to 9 PM every day.`\n",
    "\n",
    "- Second attempt with `\"\\n\"` for remaining long chunk:\n",
    "  1. **Chunk 1:** `We offer a variety of products. Our range includes electronics, clothing, and home appliances.`\n",
    "  2. **Chunk 2:** `Our staff is available to help you during store hours: 9 AM to 9 PM every day.`\n",
    "\n",
    "- No further splits are necessary as all chunks are now of manageable size.\n",
    "\n",
    "In this example, the text is initially split into two chunks using the double newline. Since one chunk is still quite long, it then uses a single newline to split it further. Each chunk retains coherent, complete information, reflecting the effective use of the recursive character splitter to preserve the semantic structure of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c9adc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f4f0f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=26,\n",
    "    chunk_overlap=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42d904ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'abcdefghijklmnopqrstuvwxyzabcdefg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7396e743",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"\"\"\n",
    "Data that Speak\n",
    "LLM Applications are revolutionizing industries such as \n",
    "banking, healthcare, insurance, education, legal, tourism, \n",
    "construction, logistics, marketing, sales, customer service, \n",
    "and even public administration.\n",
    "\n",
    "The aim of our programs is for students to learn how to \n",
    "create LLM Applications in the context of a business, \n",
    "which presents a set of challenges that are important \n",
    "to consider in advance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a2f91ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz', 'wxyzabcdefg']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recursive_splitter.split_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a4b4a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data that Speak',\n",
       " 'LLM Applications are',\n",
       " 'are revolutionizing',\n",
       " 'industries such as',\n",
       " 'banking, healthcare,',\n",
       " 'insurance, education,',\n",
       " 'legal, tourism,',\n",
       " 'construction, logistics,',\n",
       " 'marketing, sales,',\n",
       " 'customer service,',\n",
       " 'and even public',\n",
       " 'administration.',\n",
       " 'The aim of our programs',\n",
       " 'is for students to learn',\n",
       " 'how to',\n",
       " 'create LLM Applications',\n",
       " 'in the context of a',\n",
       " 'a business,',\n",
       " 'which presents a set of',\n",
       " 'of challenges that are',\n",
       " 'are important',\n",
       " 'to consider in advance.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recursive_splitter.split_text(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7756d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6acfa6-5c64-417f-a2e1-c1f9939f1305",
   "metadata": {},
   "source": [
    "The separators `[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]` refer to a sequence of characters and patterns used to split text into smaller parts, with each separator serving a specific function based on the structure of text:\n",
    "\n",
    "1. `\"\\n\\n\"` - This separator targets double newlines, which are often used to denote separate paragraphs or sections within a text. Splitting here is intended to separate distinct thematic or topical blocks of content.\n",
    "\n",
    "2. `\"\\n\"` - This targets single newline characters, which typically indicate a new line within a paragraph or a soft break in the content, such as between list items or sub-paragraphs.\n",
    "\n",
    "3. `\"(?<=\\. )\"` - This is a regular expression that looks for a period followed by a space, typically used to signify the end of a sentence. The `(?<= )` part is a \"lookbehind\" assertion in regex, which means it checks for the occurrence of a period and space before splitting, but does not include them in the split, thus keeping sentences intact.\n",
    "\n",
    "4. `\" \"` - This targets single space characters, which are commonly used to separate words. Splitting on spaces can break down text into individual words or phrases, especially useful when finer granularity is needed.\n",
    "\n",
    "5. `\"\"` - An empty string as a separator is used in text processing to split a text into its individual characters, essentially decomposing the text down to its most basic elements.\n",
    "\n",
    "Each of these separators is used to progressively split the text into smaller and smaller chunks, starting from larger structural divisions like paragraphs and moving down to the level of individual characters, depending on the needs of the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45e8a845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data that Speak\\nLLM Applications are revolutionizing industries such as \\nbanking, healthcare, insurance, education, legal, tourism,',\n",
       " 'construction, logistics, marketing, sales, customer service, \\nand even public administration.',\n",
       " 'The aim of our programs is for students to learn how to \\ncreate LLM Applications in the context of a business,',\n",
       " 'which presents a set of challenges that are important \\nto consider in advance.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_recursive_splitter.split_text(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc24f4f-d23c-4202-992d-91b0623136ae",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 002-splitters.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 002-splitters.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af248e-6069-44b3-a2cd-a20aa3259874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
